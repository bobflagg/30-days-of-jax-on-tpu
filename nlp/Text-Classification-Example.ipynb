{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b969cf",
   "metadata": {},
   "source": [
    "- [Text Classification Using Flax (JAX) Networks](https://coderzcolumn.com/tutorials/artificial-intelligence/text-classification-using-flax-jax-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170f2763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b9e0f",
   "metadata": {},
   "source": [
    "# Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d94c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"b1\":0.9, \n",
    "    \"b2\":0.999, \n",
    "    \"eps\":1e-6,\n",
    "    \"learning-rate\":2e-5,\n",
    "    \"model\":\"bert-base-cased\",\n",
    "    \"number-of-epochs\":3,\n",
    "    \"number-of-labels\":2,\n",
    "    \"train-in-parallel\":False,\n",
    "    \"per-device-batch-size\":4,\n",
    "    \"seed\":0,\n",
    "    \"weight-decay\":1e-2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2e5e91",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b126c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9861c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bbba6940ad5f6fd1\n",
      "Reusing dataset csv (/home/rflagg/.cache/huggingface/datasets/csv/default-bbba6940ad5f6fd1/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cec4a32bab34731844bb7f8a546a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"/home/rflagg/data/sst2\"\n",
    "dataset = load_dataset(\n",
    "    'csv', data_files={'train': f\"{directory}/train-df.csv\", 'test': f\"{directory}/test-df.csv\", 'validation': f\"{directory}/validation-df.csv\"})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55cc5f",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a584abfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model'])\n",
    "\n",
    "def preprocess_function(data):\n",
    "    texts = (data[\"sentence\"],)\n",
    "    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n",
    "    processed[\"labels\"] = data[\"label\"]\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae77e9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7fc6adfc3820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5e99867b6849c1a5e1783d6a1a15ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41e420e99d74c6187d0648d8afe3f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a72625caf8446b48a8069864826d861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokenized = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "dataset_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fc9ebc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_ds = dataset_tokenized[\"train\"]\n",
    "test_ds = dataset_tokenized[\"test\"]\n",
    "validation_ds = dataset_tokenized[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953f87c1",
   "metadata": {},
   "source": [
    "# Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa67b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from transformers import FlaxAutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "metric = load_metric('glue', 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6655e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c771f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing FlaxBertForSequenceClassification: {('cls', 'predictions', 'transform', 'LayerNorm', 'scale'), ('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'bias'), ('cls', 'predictions', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: {('bert', 'pooler', 'dense', 'bias'), ('classifier', 'kernel'), ('classifier', 'bias'), ('bert', 'pooler', 'dense', 'kernel')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(CONFIG['model'], num_labels=CONFIG[\"number-of-labels\"])\n",
    "model = FlaxAutoModelForSequenceClassification.from_pretrained(CONFIG['model'], config=config, seed=CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86d7845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall batch size (both for training and eval) is 4\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = CONFIG['per-device-batch-size']\n",
    "if CONFIG['train-in-parallel']: total_batch_size *= jax.local_device_count()\n",
    "print(\"The overall batch size (both for training and eval) is\", total_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371535cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_steps = len(train_ds) // total_batch_size * CONFIG['number-of-epochs']\n",
    "\n",
    "learning_rate_function = optax.linear_schedule(init_value=CONFIG['learning-rate'], end_value=0, transition_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538d634",
   "metadata": {},
   "source": [
    "## Defining the training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97994714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.training import train_state\n",
    "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb4ffb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    logits_function: Callable = flax.struct.field(pytree_node=False)\n",
    "    loss_function: Callable = flax.struct.field(pytree_node=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32cfbbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_mask_fn(params):\n",
    "    flat_params = flax.traverse_util.flatten_dict(params)\n",
    "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n",
    "    return flax.traverse_util.unflatten_dict(flat_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75be24a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw(weight_decay):\n",
    "    return optax.adamw(\n",
    "        learning_rate=learning_rate_function, b1=CONFIG['b1'], b2=CONFIG['b2'], eps=CONFIG['eps'], weight_decay=weight_decay, mask=decay_mask_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8d36180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(logits, labels):\n",
    "    xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=CONFIG['number-of-labels']))\n",
    "    return jnp.mean(xentropy)\n",
    "     \n",
    "def eval_function(logits):\n",
    "    return logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9dd14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TrainState.create(\n",
    "    apply_fn=model.__call__,\n",
    "    params=model.params,\n",
    "    tx=adamw(weight_decay=CONFIG['weight-decay']),\n",
    "    logits_function=eval_function,\n",
    "    loss_function=loss_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337afcd8",
   "metadata": {},
   "source": [
    "## Defining the training and evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "943b844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch, dropout_rng, train_in_parallel=CONFIG['train-in-parallel']):\n",
    "    targets = batch.pop(\"labels\")\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "\n",
    "    def loss_function(params):\n",
    "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
    "        loss = state.loss_function(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    grad_function = jax.value_and_grad(loss_function)\n",
    "    loss, grad = grad_function(state.params)\n",
    "    if train_in_parallel:\n",
    "        grad = jax.lax.pmean(grad, \"batch\")\n",
    "        new_state = state.apply_gradients(grads=grad)\n",
    "        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n",
    "    else:\n",
    "        new_state = state.apply_gradients(grads=grad)\n",
    "        metrics = {\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}\n",
    "        \n",
    "    return new_state, metrics, new_dropout_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e0273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(state, batch):\n",
    "    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
    "    return state.logits_function(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15698660",
   "metadata": {},
   "source": [
    "## Defining the data collators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba758e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loader(rng, dataset, batch_size, train_in_parallel=CONFIG['train-in-parallel']):\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        if train_in_parallel: batch = shard(batch)\n",
    "\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "531ad184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data_loader(dataset, batch_size, train_in_parallel=CONFIG['train-in-parallel']):\n",
    "    for i in range(len(dataset) // batch_size):\n",
    "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        if train_in_parallel: batch = shard(batch)\n",
    "\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c419b6",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f358baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_train_step = jax.jit(train_step, donate_argnums=(0,))\n",
    "jit_eval_step = jax.jit(eval_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "baf26f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(CONFIG['seed'])\n",
    "rng, dropout_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1843fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 17:48:26.430890: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n"
     ]
    }
   ],
   "source": [
    "rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "    state, train_metrics, dropout_rngs = jit_train_step(state, batch, dropout_rng)\n",
    "    break\n",
    "for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "    labels = batch.pop(\"labels\")\n",
    "    predictions = jit_eval_step(state, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06d98c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "    labels = batch.pop(\"labels\")\n",
    "    predictions = jit_eval_step(state, batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f17fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "    for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        predictions = jit_eval_step(state, batch)\n",
    "        metric.add_batch(predictions=predictions, references=labels)\n",
    "        progress_bar_eval.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.add_batch(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8711b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric.compute()\n",
    "\n",
    "loss = round(train_metrics['loss'].item(), 4)\n",
    "eval_score = round(list(eval_metric.values())[0], 4)\n",
    "metric_name = list(eval_metric.keys())[0]\n",
    "\n",
    "print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f388625",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric.compute()\n",
    "loss = round(train_metrics['loss'].item(), 4)\n",
    "eval_score = round(list(eval_metric.values())[0], 4)\n",
    "metric_name = list(eval_metric.keys())[0]\n",
    "\n",
    "print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d14bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0684414ee90b490c9d59520280fa0627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch ...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/16837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/218 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 | Train loss: 0.0112 | Eval accuracy: 0.9266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e422cda47d2742f58292a89e2628eb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/16837 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "for i, epoch in enumerate(tqdm(range(1, CONFIG['number-of-epochs'] + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "    # train\n",
    "    with tqdm(total=len(train_ds) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
    "        for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "            state, train_metrics, dropout_rng = jit_train_step(state, batch, dropout_rng)\n",
    "            progress_bar_train.update(1)\n",
    "\n",
    "    # evaluate\n",
    "    with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "        for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "            labels = batch.pop(\"labels\")\n",
    "            predictions = jit_eval_step(state, batch)\n",
    "            metric.add_batch(predictions=predictions, references=labels)\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "\n",
    "    loss = round(train_metrics['loss'].item(), 4)\n",
    "    eval_score = round(list(eval_metric.values())[0], 4)\n",
    "    metric_name = list(eval_metric.keys())[0]\n",
    "\n",
    "    print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0288a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e6f943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01f62e4",
   "metadata": {},
   "source": [
    "## Parallel Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ca12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n",
    "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
    "state = flax.jax_utils.replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(CONFIG['seed'])\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffa4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "    state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
    "    break\n",
    "for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "    labels = batch.pop(\"labels\")\n",
    "    predictions = parallel_eval_step(state, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6242c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, epoch in enumerate(tqdm(range(1, CONFIG['number-of-epochs'] + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "    # train\n",
    "    with tqdm(total=len(train_ds) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
    "      for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "        state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
    "        progress_bar_train.update(1)\n",
    "\n",
    "    # evaluate\n",
    "    with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "      for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "          labels = batch.pop(\"labels\")\n",
    "          predictions = parallel_eval_step(state, batch)\n",
    "          metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n",
    "          progress_bar_eval.update(1)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "\n",
    "    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 4)\n",
    "    eval_score = round(list(eval_metric.values())[0], 4)\n",
    "    metric_name = list(eval_metric.keys())[0]\n",
    "\n",
    "    print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190a8bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "#import gc\n",
    "\n",
    "all_categories = ['alt.atheism','comp.graphics','comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware',\n",
    "                  'comp.sys.mac.hardware','comp.windows.x', 'misc.forsale','rec.autos','rec.motorcycles',\n",
    "                  'rec.sport.baseball','rec.sport.hockey','sci.crypt','sci.electronics','sci.med',\n",
    "                  'sci.space','soc.religion.christian','talk.politics.guns','talk.politics.mideast',\n",
    "                  'talk.politics.misc','talk.religion.misc']\n",
    "\n",
    "selected_categories = ['comp.sys.mac.hardware','comp.windows.x','rec.motorcycles','sci.crypt','talk.politics.mideast']\n",
    "\n",
    "X_train_text, Y_train = datasets.fetch_20newsgroups(subset=\"train\", categories=selected_categories, return_X_y=True)\n",
    "X_test_text , Y_test  = datasets.fetch_20newsgroups(subset=\"test\", categories=selected_categories, return_X_y=True)\n",
    "\n",
    "X_train_text = np.array(X_train_text)\n",
    "X_test_text = np.array(X_test_text)\n",
    "\n",
    "classes = np.unique(Y_train)\n",
    "mapping = dict(zip(classes, selected_categories))\n",
    "\n",
    "len(X_train_text), len(X_test_text), classes, mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from jax import numpy as jnp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=50000)\n",
    "\n",
    "vectorizer.fit(np.concatenate((X_train_text, X_test_text)))\n",
    "X_train = vectorizer.transform(X_train_text)\n",
    "X_test = vectorizer.transform(X_test_text)\n",
    "\n",
    "X_train = jnp.array(X_train.toarray(), dtype=jnp.float16)\n",
    "X_test  = jnp.array(X_test.toarray(), dtype=jnp.float16)\n",
    "\n",
    "X_train.shape, X_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea93ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen\n",
    "from jax import random\n",
    "\n",
    "class TextClassifier(linen.Module):\n",
    "    def setup(self):\n",
    "        self.linear1 = linen.Dense(features=128, name=\"DENSE1\")\n",
    "        self.linear2 = linen.Dense(features=64, name=\"DENSE2\")\n",
    "        self.linear3 = linen.Dense(len(classes), name=\"DENSE3\")\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = linen.relu(self.linear1(inputs))\n",
    "        x = linen.relu(self.linear2(x))\n",
    "        logits = self.linear3(x)\n",
    "\n",
    "        return logits #linen.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dc8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = jax.random.PRNGKey(0)\n",
    "\n",
    "model = TextClassifier()\n",
    "params = model.init(seed, X_train[:5])\n",
    "\n",
    "for layer_params in params[\"params\"].items():\n",
    "    print(\"Layer Name : {}\".format(layer_params[0]))\n",
    "    weights, biases = layer_params[1][\"kernel\"], layer_params[1][\"bias\"]\n",
    "    print(\"\\tLayer Weights : {}, Biases : {}\".format(weights.shape, biases.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.apply(params, X_train[:5])\n",
    "\n",
    "preds.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1841dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(weights, input_data, actual):\n",
    "    logits_preds = model.apply(weights, input_data)\n",
    "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
    "    return optax.softmax_cross_entropy(logits=logits_preds, labels=one_hot_actual).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c6e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def TrainModelInBatches(X, Y, X_val, Y_val, epochs, weights, optimizer_state, batch_size=32):\n",
    "    for i in range(1, epochs+1):\n",
    "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
    "\n",
    "        losses = [] ## Record loss of each batch\n",
    "        for batch in tqdm(batches):\n",
    "            if batch != batches[-1]:\n",
    "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
    "            else:\n",
    "                start, end = int(batch*batch_size), None\n",
    "\n",
    "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
    "\n",
    "            loss, gradients = value_and_grad(CrossEntropyLoss)(weights, X_batch,Y_batch)\n",
    "\n",
    "            ## Update Weights\n",
    "            updates, optimizer_state = optimizer.update(gradients, optimizer_state)\n",
    "            weights = optax.apply_updates(weights, updates)\n",
    "\n",
    "            losses.append(loss) ## Record Loss\n",
    "\n",
    "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
    "\n",
    "        Y_val_preds = model.apply(weights, X_val)\n",
    "        val_acc = accuracy_score(Y_val, jnp.argmax(Y_val_preds, axis=1))\n",
    "        print(\"Validation  Accuracy : {:.3f}\".format(val_acc))\n",
    "\n",
    "    return weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe04cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = random.PRNGKey(0)\n",
    "batch_size=256\n",
    "epochs=8\n",
    "learning_rate = jnp.array(1/1e3)\n",
    "\n",
    "model = TextClassifier()\n",
    "weights = model.init(seed, X_train[:5])\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate)\n",
    "optimizer_state = optimizer.init(weights)\n",
    "\n",
    "final_weights = TrainModelInBatches(X_train, Y_train, X_test, Y_test, epochs, weights, optimizer_state, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221c43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
