{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b969cf",
   "metadata": {},
   "source": [
    "- [Text Classification Using Flax (JAX) Networks](https://coderzcolumn.com/tutorials/artificial-intelligence/text-classification-using-flax-jax-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "170f2763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ae0c49",
   "metadata": {},
   "source": [
    "# Configure Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20782bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"b1\":0.9, \n",
    "    \"b2\":0.999, \n",
    "    \"dataset\":\"BBC-News\", # BBC-News, sst2\n",
    "    \"eps\":1e-6,\n",
    "    \"learning-rate\":2e-5,\n",
    "    \"model\":\"bert-base-cased\",\n",
    "    \"model-directory\":f\"/home/rflagg/model/BBC-News\",\n",
    "    \"number-of-epochs\":5,\n",
    "    \"number-of-labels\":5,# BBC-News: 5, sst2: 2\n",
    "    \"train-in-parallel\":True,\n",
    "    \"per-device-batch-size\":4,\n",
    "    \"seed\":0,\n",
    "    \"weight-decay\":1e-2,\n",
    "    \"text-key\":\"text\",  # text for BBC-News; sentence for sst2\n",
    "    \"label-key\":\"label\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48231d3f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed2d297",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-07cfe4aff386be8d\n",
      "Reusing dataset csv (/home/rflagg/.cache/huggingface/datasets/csv/default-07cfe4aff386be8d/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e5f023c6914eee8b92bbd58438cd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['category', 'text', 'label'],\n",
       "        num_rows: 2002\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['category', 'text', 'label'],\n",
       "        num_rows: 223\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "directory = f\"/home/rflagg/data/{CONFIG['dataset']}\"\n",
    "\n",
    "if CONFIG['dataset'] == \"BBC-News\":   \n",
    "    dataset = load_dataset(\n",
    "        'csv', \n",
    "        data_files={\n",
    "            'train': f\"{directory}/train-df.csv\", \n",
    "             'validation': f\"{directory}/test-df.csv\"\n",
    "        }\n",
    "    )\n",
    "else:\n",
    "    dataset = load_dataset(\n",
    "        'csv', \n",
    "        data_files={\n",
    "            'train': f\"{directory}/train-df.csv\", \n",
    "            'test': f\"{directory}/test-df.csv\", \n",
    "            'validation': f\"{directory}/validation-df.csv\"\n",
    "        }\n",
    "    )\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28d69db",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e1a984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7f330de34820> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483d0fa8fda2425ba147cfeb29b1584a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32cca2569d0d48eeaf2e86c247ab8064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model'])\n",
    "\n",
    "def preprocess_function(data, text_key=CONFIG[\"text-key\"], label_key=CONFIG[\"label-key\"]):\n",
    "    texts = (data[text_key],)\n",
    "    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n",
    "    processed[\"labels\"] = data[label_key]\n",
    "    return processed\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "dataset_tokenized\n",
    "\n",
    "train_ds = dataset_tokenized[\"train\"]\n",
    "validation_ds = dataset_tokenized[\"validation\"]\n",
    "if CONFIG['dataset'] == \"sst2\": test_ds = dataset_tokenized[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5909204f",
   "metadata": {},
   "source": [
    "# Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a3fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "#from datasets import load_metric\n",
    "#metric = load_metric('glue', \"sst2\")\n",
    "\n",
    "class F1EtcMetric(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=\"Calculates precision, recall, f1 score, and support.\",\n",
    "            citation=\"TODO: _CITATION\",\n",
    "            inputs_description=\"_KWARGS_DESCRIPTION\",\n",
    "            features=datasets.Features({\n",
    "                'predictions': datasets.Value('int64'),\n",
    "                'references': datasets.Value('int64'),\n",
    "            }),\n",
    "            codebase_urls=[],\n",
    "            reference_urls=[],\n",
    "            format='numpy'\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(references, predictions, average='weighted')\n",
    "        return {\n",
    "            \"precision\":precision,\n",
    "            \"recall\":recall,\n",
    "            \"f1\":fscore\n",
    "        }\n",
    "    \n",
    "metric = F1EtcMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1a91c",
   "metadata": {},
   "source": [
    "# Fine-tune the model\n",
    "\n",
    "- [Huggingface Evaluate](https://huggingface.co/docs/evaluate/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd70220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
    "from itertools import chain\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import FlaxAutoModelForSequenceClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55791e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing FlaxBertForSequenceClassification: {('cls', 'predictions', 'transform', 'LayerNorm', 'scale'), ('cls', 'predictions', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'bias')}\n",
      "- This IS expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: {('bert', 'pooler', 'dense', 'kernel'), ('classifier', 'bias'), ('classifier', 'kernel'), ('bert', 'pooler', 'dense', 'bias')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(CONFIG['model'], num_labels=CONFIG[\"number-of-labels\"])\n",
    "model = FlaxAutoModelForSequenceClassification.from_pretrained(CONFIG['model'], config=config, seed=CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26fbbfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall batch size (both for training and eval) is 32\n"
     ]
    }
   ],
   "source": [
    "total_batch_size = CONFIG['per-device-batch-size']\n",
    "if CONFIG['train-in-parallel']: total_batch_size *= jax.local_device_count()\n",
    "print(\"The overall batch size (both for training and eval) is\", total_batch_size)\n",
    "\n",
    "num_train_steps = len(train_ds) // total_batch_size * CONFIG['number-of-epochs']\n",
    "learning_rate_function = optax.linear_schedule(init_value=CONFIG['learning-rate'], end_value=0, transition_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b022fc9",
   "metadata": {},
   "source": [
    "## Defining the training state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "840887f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.training import train_state\n",
    "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ed58c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    logits_function: Callable = flax.struct.field(pytree_node=False)\n",
    "    loss_function: Callable = flax.struct.field(pytree_node=False)\n",
    "        \n",
    "def decay_mask_fn(params):\n",
    "    flat_params = flax.traverse_util.flatten_dict(params)\n",
    "    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n",
    "    return flax.traverse_util.unflatten_dict(flat_mask)\n",
    "\n",
    "def adamw(weight_decay):\n",
    "    return optax.adamw(\n",
    "        learning_rate=learning_rate_function, b1=CONFIG['b1'], b2=CONFIG['b2'], eps=CONFIG['eps'], weight_decay=weight_decay, mask=decay_mask_fn\n",
    "    )\n",
    "\n",
    "def loss_function(logits, labels):\n",
    "    xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=CONFIG['number-of-labels']))\n",
    "    return jnp.mean(xentropy)\n",
    "     \n",
    "def eval_function(logits): return logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad4d1b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = TrainState.create(\n",
    "    apply_fn=model.__call__,\n",
    "    params=model.params,\n",
    "    tx=adamw(weight_decay=CONFIG['weight-decay']),\n",
    "    logits_function=eval_function,\n",
    "    loss_function=loss_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f395c5",
   "metadata": {},
   "source": [
    "## Defining the training and evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "725cd71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch, dropout_rng, train_in_parallel=CONFIG['train-in-parallel']):\n",
    "    targets = batch.pop(\"labels\")\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "\n",
    "    def loss_function(params):\n",
    "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
    "        loss = state.loss_function(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    grad_function = jax.value_and_grad(loss_function)\n",
    "    loss, grad = grad_function(state.params)\n",
    "    if train_in_parallel:\n",
    "        grad = jax.lax.pmean(grad, \"batch\")\n",
    "        new_state = state.apply_gradients(grads=grad)\n",
    "        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n",
    "    else:\n",
    "        new_state = state.apply_gradients(grads=grad)\n",
    "        metrics = {\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}\n",
    "        \n",
    "    return new_state, metrics, new_dropout_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43271f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(state, batch):\n",
    "    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
    "    return state.logits_function(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf7031",
   "metadata": {},
   "source": [
    "## Defining the data collators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f456fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_loader(rng, dataset, batch_size, train_in_parallel=CONFIG['train-in-parallel']):\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        if train_in_parallel: batch = shard(batch)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccabe92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_data_loader(dataset, batch_size, train_in_parallel=CONFIG['train-in-parallel']):\n",
    "    for i in range(len(dataset) // batch_size):\n",
    "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        if train_in_parallel: batch = shard(batch)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219ecef",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_train_step = jax.jit(train_step, donate_argnums=(0,))\n",
    "jit_eval_step = jax.jit(eval_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73632127",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(CONFIG['seed'])\n",
    "rng, dropout_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "    state, train_metrics, dropout_rngs = jit_train_step(state, batch, dropout_rng)\n",
    "    break\n",
    "for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "    labels = batch.pop(\"labels\")\n",
    "    predictions = jit_eval_step(state, batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5417e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "    labels = batch.pop(\"labels\")\n",
    "    predictions = jit_eval_step(state, batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d849da",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "    for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        predictions = jit_eval_step(state, batch)\n",
    "        metric.add_batch(predictions=predictions, references=labels)\n",
    "        progress_bar_eval.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e250d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbdc1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric.compute()\n",
    "\n",
    "loss = round(train_metrics['loss'].item(), 4)\n",
    "eval_score = round(list(eval_metric.values())[0], 4)\n",
    "metric_name = list(eval_metric.keys())[0]\n",
    "\n",
    "print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee02d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric.compute()\n",
    "loss = round(train_metrics['loss'].item(), 4)\n",
    "eval_score = round(list(eval_metric.values())[0], 4)\n",
    "metric_name = list(eval_metric.keys())[0]\n",
    "\n",
    "print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2ac5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax\n",
    "flax.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186bf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i, epoch in enumerate(tqdm(range(1, CONFIG['number-of-epochs'] + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "    # train\n",
    "    with tqdm(total=len(train_ds) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
    "        for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "            state, train_metrics, dropout_rng = jit_train_step(state, batch, dropout_rng)\n",
    "            progress_bar_train.update(1)\n",
    "\n",
    "    # evaluate\n",
    "    with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "        for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "            labels = batch.pop(\"labels\")\n",
    "            predictions = jit_eval_step(state, batch)\n",
    "            metric.add_batch(predictions=predictions, references=labels)\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "\n",
    "    loss = round(train_metrics['loss'].item(), 4)\n",
    "    eval_score = round(list(eval_metric.values())[2], 4)\n",
    "    metric_name = list(eval_metric.keys())[2]\n",
    "\n",
    "    print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss} | Eval {metric_name}: {100 * eval_score:02f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c704b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(CONFIG['model-directory'])\n",
    "model.save_pretrained(CONFIG['model-directory'], state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec813e15",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model-directory'])\n",
    "config = AutoConfig.from_pretrained(CONFIG['model-directory'], num_labels=CONFIG[\"number-of-labels\"])\n",
    "model = FlaxAutoModelForSequenceClassification.from_pretrained(CONFIG['model-directory'], config=config, seed=CONFIG['seed'])\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.__call__,\n",
    "    params=model.params,\n",
    "    tx=adamw(weight_decay=CONFIG['weight-decay']),\n",
    "    logits_function=eval_function,\n",
    "    loss_function=loss_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123726c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "    for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        predictions = jit_eval_step(state, batch)\n",
    "        metric.add_batch(predictions=predictions, references=labels)\n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "test_metric = metric.compute()\n",
    "\n",
    "test_score = round(list(test_metric.values())[0], 4)\n",
    "metric_name = list(test_metric.keys())[0]\n",
    "\n",
    "print(f\"Test {metric_name}: {100 * test_score:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b3b87",
   "metadata": {},
   "source": [
    "## Parallel Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ed1e8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n",
    "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n",
    "state = flax.jax_utils.replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3e246de",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(CONFIG['seed'])\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8885acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.19254032258064516, 'recall': 0.21875, 'f1': 0.11289414414414414}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rflagg/anaconda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "    state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
    "    break\n",
    "for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "    labels = batch.pop(\"labels\")\n",
    "    predictions = parallel_eval_step(state, batch)\n",
    "    metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n",
    "    eval_metric = metric.compute()\n",
    "    print(eval_metric)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96697350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12233c4d0ab4d939a2c1cd85a81a24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch ...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 | Train loss: 0.0051 | Eval f1: 95.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 | Train loss: 0.0034 | Eval f1: 95.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/5 | Train loss: 0.0058 | Eval f1: 95.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/5 | Train loss: 0.0038 | Eval f1: 95.80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 | Train loss: 0.0040 | Eval f1: 95.80\n",
      "CPU times: user 50.9 s, sys: 8.07 s, total: 58.9 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, epoch in enumerate(tqdm(range(1, CONFIG['number-of-epochs'] + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "    # train\n",
    "    with tqdm(total=len(train_ds) // total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
    "        for batch in train_data_loader(input_rng, train_ds, total_batch_size):\n",
    "            state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
    "            progress_bar_train.update(1)\n",
    "\n",
    "    # evaluate\n",
    "    with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "        for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "            labels = batch.pop(\"labels\")\n",
    "            predictions = parallel_eval_step(state, batch)\n",
    "            metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "\n",
    "    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 4)\n",
    "    eval_score = round(list(eval_metric.values())[2], 4)\n",
    "    metric_name = list(eval_metric.keys())[2]\n",
    "    #eval_score = round(list(eval_metric.values())[0], 4)\n",
    "    #metric_name = list(eval_metric.keys())[0]\n",
    "\n",
    "    print(f\"{i+1}/{CONFIG['number-of-epochs']} | Train loss: {loss:0.4f} | Eval {metric_name}: {100 * eval_score:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2221c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(CONFIG['model-directory'])\n",
    "model.save_pretrained(CONFIG['model-directory'], flax.jax_utils.unreplicate(state.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999d3d8d",
   "metadata": {},
   "source": [
    "### Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56873457",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG['model-directory'])\n",
    "config = AutoConfig.from_pretrained(CONFIG['model-directory'], num_labels=CONFIG[\"number-of-labels\"])\n",
    "model = FlaxAutoModelForSequenceClassification.from_pretrained(CONFIG['model-directory'], config=config, seed=CONFIG['seed'])\n",
    "\n",
    "state = TrainState.create(\n",
    "    apply_fn=model.__call__,\n",
    "    params=model.params,\n",
    "    tx=adamw(weight_decay=CONFIG['weight-decay']),\n",
    "    logits_function=eval_function,\n",
    "    loss_function=loss_function,\n",
    ")\n",
    "state = flax.jax_utils.replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d82acd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 95.90%\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(validation_ds) // total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "    for batch in eval_data_loader(validation_ds, total_batch_size):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        predictions = parallel_eval_step(state, batch)\n",
    "        metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n",
    "        progress_bar_eval.update(1)\n",
    "\n",
    "test_metric = metric.compute()\n",
    "\n",
    "test_score = round(list(test_metric.values())[0], 4)\n",
    "metric_name = list(test_metric.keys())[0]\n",
    "\n",
    "print(f\"Test {metric_name}: {100 * test_score:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176da092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
