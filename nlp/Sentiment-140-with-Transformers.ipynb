{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b374f71",
   "metadata": {},
   "source": [
    "- [Sentiment CLF - JAX/FLAX on TPUs + ðŸ¤—](https://www.kaggle.com/code/heyytanay/sentiment-clf-jax-flax-on-tpus-w-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "620d070f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b58ed12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using jax 0.3.13\n",
      "Optax Version : 0.1.2\n",
      "Found 8 devices.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import jax\n",
    "from jax import numpy as jnp, random\n",
    "import numpy as np\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "print(\"Using jax\", jax.__version__)\n",
    "print(\"Optax Version : {}\".format(optax.__version__))\n",
    "devices = jax.local_devices()\n",
    "print(f\"Found {len(devices)} devices.\")\n",
    "devices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3755dc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "import flax\n",
    "\n",
    "from itertools import chain\n",
    "from typing import Callable\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
    "from flax.training import train_state\n",
    "from flax import traverse_util\n",
    "\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "jax.local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64d25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-20 21:28:03.877477: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-06-20 21:28:03.877519: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_state\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflax\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m traverse_util\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, FlaxAutoModelForSequenceClassification, AutoConfig\n\u001b[1;32m     23\u001b[0m jax\u001b[38;5;241m.\u001b[39mlocal_devices()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import jax\n",
    "import flax\n",
    "import optax\n",
    "import jaxlib\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from itertools import chain\n",
    "from typing import Callable\n",
    "\n",
    "from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n",
    "from flax.training import train_state\n",
    "from flax import traverse_util\n",
    "\n",
    "from transformers import AutoTokenizer, FlaxAutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "jax.local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "070189cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee4d6f80f6e408c8b84a2d70cc92525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb12bb26724443c975c57b882ef9854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bfe268c7a3417a9ee03c7763d41a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79c20e49deb4474cba3e7f3541688ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Config:\n",
    "    nb_epochs = 5\n",
    "    lr = 2e-5\n",
    "    per_device_bs = 32\n",
    "    num_labels = 2\n",
    "    model_name = 'bert-base-uncased'\n",
    "    total_batch_size = per_device_bs * jax.local_device_count()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "CONFIG = dict(\n",
    "    lr=2e-5,\n",
    "    model_name = 'bert-base-uncased',\n",
    "    epochs = 5,\n",
    "    split = 0.10,\n",
    "    per_device_bs = 32,\n",
    "    seed = 42,\n",
    "    num_labels = 2,\n",
    "    infra = \"Kaggle\",\n",
    "    competition = 'none',\n",
    "    _wandb_kernel = 'tanaym'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b109f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_acc(preds, labels):\n",
    "    assert len(preds) == len(labels), \"Predictions and Labels matrices must be of same length\"\n",
    "    acc = (preds == labels).sum() / len(preds)\n",
    "    return acc\n",
    "\n",
    "class ACCURACY(datasets.Metric):\n",
    "    def _info(self):\n",
    "        return datasets.MetricInfo(\n",
    "            description=\"Calculates Accuracy metric.\",\n",
    "            citation=\"TODO: _CITATION\",\n",
    "            inputs_description=\"_KWARGS_DESCRIPTION\",\n",
    "            features=datasets.Features({\n",
    "                'predictions': datasets.Value('int64'),\n",
    "                'references': datasets.Value('int64'),\n",
    "            }),\n",
    "            codebase_urls=[],\n",
    "            reference_urls=[],\n",
    "            format='numpy'\n",
    "        )\n",
    "\n",
    "    def _compute(self, predictions, references):\n",
    "        return {\"ACCURACY\": simple_acc(predictions, references)}\n",
    "    \n",
    "metric = ACCURACY()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec3a9be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training.1600000.processed.noemoticon.csv\r\n",
      "training.1600000.processed.noemoticon.csv.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rflagg/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8af127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def split_and_save(file_path: str, split: float = 0.10):\n",
    "    file = pd.read_csv(file_path, encoding='latin-1', names=['sentiment', 'id', 'date', 'query', 'username', 'text'])\n",
    "    file = file[['sentiment', 'text']]\n",
    "    file['sentiment'] = file['sentiment'].map({4: 1, 0: 0})\n",
    "    \n",
    "    file = file.sample(frac=1).reset_index(drop=True)\n",
    "    split_nb = int(len(file) * split)\n",
    "    \n",
    "    test_set = file[:split_nb].reset_index(drop=True)\n",
    "    train_set = file[split_nb:].reset_index(drop=True)\n",
    "    \n",
    "    train_set.to_csv(\"train_file.csv\", index=None)\n",
    "    test_set.to_csv(\"test_file.csv\", index=None)\n",
    "    print(\"Done.\")\n",
    "\n",
    "split_and_save(\"/home/rflagg/data/training.1600000.processed.noemoticon.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf336dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-f1e7f08119a1ffda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/rflagg/.cache/huggingface/datasets/csv/default-f1e7f08119a1ffda/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee7d61da7c246068b6919e959c3889a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de61d5a3e3545508dcd8665b05a208c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/rflagg/.cache/huggingface/datasets/csv/default-f1e7f08119a1ffda/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f1ce04b2224587aed15cf7db0f8269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration default-ea959330fd406536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/rflagg/.cache/huggingface/datasets/csv/default-ea959330fd406536/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c4329b0cb343208c45b01c33cc1715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c138b69b63484392f393f3a9f9f84b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/rflagg/.cache/huggingface/datasets/csv/default-ea959330fd406536/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a413a6469527468c95ab6ad3808ca2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the training and testing files loaded in HF dataset format\n",
    "raw_train = load_dataset(\"csv\", data_files={'train': ['./train_file.csv']})\n",
    "raw_test = load_dataset(\"csv\", data_files={'test': ['./test_file.csv']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de2e928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    \"\"\"\n",
    "    Preprocessing function\n",
    "    \"\"\"\n",
    "    texts = (data[\"text\"],)\n",
    "    processed = Config.tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n",
    "    processed[\"labels\"] = data[\"sentiment\"]\n",
    "    return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04e264be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.fingerprint:Parameter 'function'=<function preprocess_function at 0x7f8a935f3a60> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e375ad48d54d29859729deb0e710ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1440 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d5947920a84267931272028ffc9c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/160 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 28s, sys: 8min 59s, total: 21min 28s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_dataset = raw_train.map(preprocess_function, batched=True, remove_columns=raw_train[\"train\"].column_names)\n",
    "test_dataset = raw_test.map(preprocess_function, batched=True, remove_columns=raw_test['test'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54061e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1440000 160000\n"
     ]
    }
   ],
   "source": [
    "train = train_dataset['train']\n",
    "valid = test_dataset['test']\n",
    "print(len(train), len(valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cbd22f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0211d134c72343b3a48042f4badc1da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing FlaxBertForSequenceClassification: {('cls', 'predictions', 'transform', 'LayerNorm', 'bias'), ('cls', 'predictions', 'bias'), ('cls', 'predictions', 'transform', 'dense', 'kernel'), ('cls', 'predictions', 'transform', 'dense', 'bias'), ('cls', 'predictions', 'transform', 'LayerNorm', 'scale')}\n",
      "- This IS expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaxBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FlaxBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: {('bert', 'pooler', 'dense', 'bias'), ('classifier', 'bias'), ('bert', 'pooler', 'dense', 'kernel'), ('classifier', 'kernel')}\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(Config.model_name, num_labels=Config.num_labels)\n",
    "model = FlaxAutoModelForSequenceClassification.from_pretrained(Config.model_name, config=config, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60169f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train steps (all the epochs) is 28125\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = len(train) // Config.total_batch_size * Config.nb_epochs\n",
    "learning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=Config.lr, pct_start=0.1)\n",
    "print(\"The number of train steps (all the epochs) is\", num_train_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "303faef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adamw(learning_rate=Config.lr, b1=0.9, b2=0.999, eps=1e-6, weight_decay=1e-2)\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    loss = optax.softmax_cross_entropy(logits, onehot(targets, num_classes=Config.num_labels))\n",
    "    return jnp.mean(loss)\n",
    "def eval_fn(logits):\n",
    "    return logits.argmax(-1)\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    eval_function: Callable = flax.struct.field(pytree_node=False)\n",
    "    loss_function: Callable = flax.struct.field(pytree_node=False)\n",
    "        \n",
    "state = TrainState.create(\n",
    "    apply_fn = model.__call__,\n",
    "    params = model.params,\n",
    "    tx = optimizer,\n",
    "    eval_function=eval_fn,\n",
    "    loss_function=loss_fn,\n",
    ")\n",
    "\n",
    "def train_step(state, batch, dropout_rng):\n",
    "    targets = batch.pop(\"labels\")\n",
    "    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n",
    "    \n",
    "    def loss_function(params):\n",
    "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
    "        loss = state.loss_function(logits, targets)\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_function)\n",
    "    loss, grad = grad_fn(state.params)\n",
    "    grad = jax.lax.pmean(grad, \"batch\")\n",
    "    new_state = state.apply_gradients(grads=grad)\n",
    "    metrics = jax.lax.pmean({'loss': loss, 'learning_rate': learning_rate_function(state.step)}, axis_name='batch')\n",
    "    \n",
    "    return new_state, metrics, new_dropout_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34536cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "515f0292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(state, batch):\n",
    "    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n",
    "    return state.eval_function(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa662017",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b160c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentTrainDataLoader(rng, dataset, batch_size):\n",
    "    steps_per_epoch = len(dataset) // batch_size\n",
    "    perms = jax.random.permutation(rng, len(dataset))\n",
    "    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = dataset[perm]\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5cd9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimentEvalDataLoader(dataset, batch_size):\n",
    "    for i in range(len(dataset) // batch_size):\n",
    "        batch = dataset[i * batch_size : (i + 1) * batch_size]\n",
    "        batch = {k: jnp.array(v) for k, v in batch.items()}\n",
    "        batch = shard(batch)\n",
    "\n",
    "        yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44bb3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = flax.jax_utils.replicate(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6a9c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(42)\n",
    "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7902c655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3820129c07894435847c59b8cc21bb3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5b5de9dfb64db9bd7ae5da81591571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/5625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, epoch in enumerate(tqdm(range(1, Config.nb_epochs + 1), desc=f\"Epoch...\", position=0, leave=True)):\n",
    "    rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "    # train\n",
    "    with tqdm(total=len(train) // Config.total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n",
    "        for batch in sentimentTrainDataLoader(input_rng, train, Config.total_batch_size):\n",
    "            state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n",
    "            progress_bar_train.update(1)\n",
    "\n",
    "    # evaluate\n",
    "    with tqdm(total=len(valid) // Config.total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n",
    "        for batch in sentimentEvalDataLoader(valid, Config.total_batch_size):\n",
    "            labels = batch.pop(\"labels\")\n",
    "            predictions = parallel_eval_step(state, batch)\n",
    "            metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n",
    "            progress_bar_eval.update(1)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "\n",
    "    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n",
    "    eval_score = round(list(eval_metric.values())[0], 3)\n",
    "    metric_name = list(eval_metric.keys())[0]\n",
    "    \n",
    "    print(f\"{i+1}/{Config.nb_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d317a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
