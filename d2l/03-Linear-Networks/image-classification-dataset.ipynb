{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# The Image Classification Dataset\n",
    ":label:`sec_fashion_mnist`\n",
    "\n",
    "(~~The MNIST dataset is one of the widely used dataset for image classification, while it's too simple as a benchmark dataset. We will use the similar, but more complex Fashion-MNIST dataset~~)\n",
    "\n",
    "One of the widely used dataset for image classification is the  MNIST dataset :cite:`LeCun.Bottou.Bengio.ea.1998`.\n",
    "While it had a good run as a benchmark dataset,\n",
    "even simple models by today's standards achieve classification accuracy over 95%,\n",
    "making it unsuitable for distinguishing between stronger models and weaker ones.\n",
    "Today, MNIST serves as more of sanity checks than as a benchmark.\n",
    "To up the ante just a bit, we will focus our discussion in the coming sections\n",
    "on the qualitatively similar, but comparatively complex Fashion-MNIST\n",
    "dataset :cite:`Xiao.Rasul.Vollgraf.2017`, which was released in 2017.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using jax 0.3.13\n",
      "Optax Version : 0.1.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import d2l\n",
    "d2l.use_svg_display()\n",
    "import jax\n",
    "from jax import numpy as jnp, random\n",
    "import numpy as np\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "print(\"Using jax\", jax.__version__)\n",
    "print(\"Optax Version : {}\".format(optax.__version__))\n",
    "jax.local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from d2l import torch as d2l\n",
    "\n",
    "d2l.use_svg_display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "## Reading the Dataset\n",
    "\n",
    "We can [**download and read the Fashion-MNIST dataset into memory via the build-in functions in the framework.**]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 22:28:27.311803: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1), (60000,), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from jax import numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train, X_test = X_train.reshape(-1,28,28,1), X_test.reshape(-1,28,28,1)\n",
    "\n",
    "X_train, X_test = jnp.array(X_train), jnp.array(X_test)\n",
    "\n",
    "X_train, X_test = X_train/255.0, X_test/255.0\n",
    "\n",
    "classes =  np.unique(Y_train)\n",
    "class_labels = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n",
    "mapping = dict(zip(classes, class_labels))\n",
    "\n",
    "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen\n",
    "from jax import random\n",
    "\n",
    "class CNN(linen.Module):\n",
    "    def setup(self):\n",
    "        self.conv1 = linen.Conv(features=32, kernel_size=(3,3), padding=\"SAME\", name=\"CONV1\")\n",
    "        self.conv2 = linen.Conv(features=16, kernel_size=(3,3), padding=\"SAME\", name=\"CONV2\")\n",
    "        self.linear1 = linen.Dense(len(classes), name=\"DENSE\")\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = linen.relu(self.conv1(inputs))\n",
    "        x = linen.relu(self.conv2(x))\n",
    "\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        logits = self.linear1(x)\n",
    "\n",
    "        return logits #linen.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name : CONV1\n",
      "\tLayer Weights : (3, 3, 1, 32), Biases : (32,)\n",
      "Layer Name : CONV2\n",
      "\tLayer Weights : (3, 3, 32, 16), Biases : (16,)\n",
      "Layer Name : DENSE\n",
      "\tLayer Weights : (12544, 10), Biases : (10,)\n"
     ]
    }
   ],
   "source": [
    "seed = jax.random.PRNGKey(0)\n",
    "\n",
    "model = CNN()\n",
    "params = model.init(seed, X_train[:5])\n",
    "\n",
    "for layer_params in params[\"params\"].items():\n",
    "    print(\"Layer Name : {}\".format(layer_params[0]))\n",
    "    weights, biases = layer_params[1][\"kernel\"], layer_params[1][\"bias\"]\n",
    "    print(\"\\tLayer Weights : {}, Biases : {}\".format(weights.shape, biases.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.apply(params, X_train[:5])\n",
    "\n",
    "preds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropyLoss(weights, input_data, actual):\n",
    "    logits = model.apply(weights, input_data)\n",
    "    one_hot_actual = jax.nn.one_hot(actual, num_classes=len(classes))\n",
    "    return optax.softmax_cross_entropy(logits, one_hot_actual).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def data_iter(X, Y, batch_size):\n",
    "    batches = jnp.arange((X.shape[0] // batch_size) + 1)\n",
    "    for batch in tqdm(batches):\n",
    "        if batch != batches[-1]: start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
    "        else: start, end = int(batch*batch_size), None\n",
    "        yield X[start:end], Y[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def TrainModelInBatches(X, Y, X_val, Y_val, epochs, weights, optimizer_state, batch_size=32):\n",
    "    for i in range(1, epochs + 1):\n",
    "        losses = [] ## Record loss of each batch\n",
    "        for X_batch, Y_batch in data_iter(X, Y, batch_size):\n",
    "            loss, gradients = value_and_grad(CrossEntropyLoss)(weights, X_batch,Y_batch)\n",
    "            ## Update Weights\n",
    "            updates, optimizer_state = optimizer.update(gradients, optimizer_state)\n",
    "            weights = optax.apply_updates(weights, updates)\n",
    "            losses.append(loss) ## Record Loss\n",
    "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
    "        Y_val_preds = model.apply(weights, X_val)\n",
    "        val_acc = accuracy_score(Y_val, jnp.argmax(Y_val_preds, axis=1))\n",
    "        print(\"Validation  Accuracy : {:.3f}\".format(val_acc))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import value_and_grad\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def TrainModelInBatches(X, Y, X_val, Y_val, epochs, weights, optimizer_state, batch_size=32):\n",
    "    for i in range(1, epochs+1):\n",
    "        batches = jnp.arange((X.shape[0]//batch_size)+1) ### Batch Indices\n",
    "\n",
    "        losses = [] ## Record loss of each batch\n",
    "        for batch in tqdm(batches):\n",
    "            if batch != batches[-1]:\n",
    "                start, end = int(batch*batch_size), int(batch*batch_size+batch_size)\n",
    "            else:\n",
    "                start, end = int(batch*batch_size), None\n",
    "\n",
    "            X_batch, Y_batch = X[start:end], Y[start:end] ## Single batch of data\n",
    "\n",
    "            loss, gradients = value_and_grad(CrossEntropyLoss)(weights, X_batch,Y_batch)\n",
    "\n",
    "            ## Update Weights\n",
    "            updates, optimizer_state = optimizer.update(gradients, optimizer_state)\n",
    "            weights = optax.apply_updates(weights, updates)\n",
    "\n",
    "            losses.append(loss) ## Record Loss\n",
    "\n",
    "        print(\"CrossEntropyLoss : {:.3f}\".format(jnp.array(losses).mean()))\n",
    "\n",
    "        Y_val_preds = model.apply(weights, X_val)\n",
    "        val_acc = accuracy_score(Y_val, jnp.argmax(Y_val_preds, axis=1))\n",
    "        print(\"Validation  Accuracy : {:.3f}\".format(val_acc))\n",
    "\n",
    "    return weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [00:32<00:00,  7.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss : 214.436\n",
      "Validation  Accuracy : 0.819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [00:25<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss : 114.604\n",
      "Validation  Accuracy : 0.844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [00:25<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss : 102.902\n",
      "Validation  Accuracy : 0.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [00:25<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss : 96.710\n",
      "Validation  Accuracy : 0.861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 235/235 [00:25<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss : 92.409\n",
      "Validation  Accuracy : 0.866\n"
     ]
    }
   ],
   "source": [
    "seed = random.PRNGKey(0)\n",
    "batch_size=256\n",
    "epochs=5\n",
    "learning_rate = jnp.array(1/1e4)\n",
    "\n",
    "model = CNN()\n",
    "weights = model.init(seed, X_train[:5])\n",
    "\n",
    "optimizer = optax.adam(learning_rate=learning_rate) ## Initialize SGD Optimizer\n",
    "optimizer_state = optimizer.init(weights)\n",
    "\n",
    "final_weights = TrainModelInBatches(X_train, Y_train, X_test, Y_test, epochs, weights, optimizer_state, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.8655\n",
      "\n",
      "Confusion Matrix : \n",
      "[[882   0  15  36   8   3  44   0  12   0]\n",
      " [  4 963   4  21   3   0   3   0   2   0]\n",
      " [ 25   0 797  10 112   1  50   0   5   0]\n",
      " [ 37  10   6 893  35   0  18   0   1   0]\n",
      " [  1   1  85  31 829   0  49   0   4   0]\n",
      " [  0   0   0   1   0 955   0  26   1  17]\n",
      " [209   1 108  29 130   1 509   0  13   0]\n",
      " [  0   0   0   0   0  37   0 902   1  60]\n",
      " [  4   0   5   9   4   1  13   5 959   0]\n",
      " [  0   0   0   0   0   4   1  28   1 966]]\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " T-shirt/top       0.76      0.88      0.82      1000\n",
      "     Trouser       0.99      0.96      0.98      1000\n",
      "    Pullover       0.78      0.80      0.79      1000\n",
      "       Dress       0.87      0.89      0.88      1000\n",
      "        Coat       0.74      0.83      0.78      1000\n",
      "      Sandal       0.95      0.95      0.95      1000\n",
      "       Shirt       0.74      0.51      0.60      1000\n",
      "     Sneaker       0.94      0.90      0.92      1000\n",
      "         Bag       0.96      0.96      0.96      1000\n",
      "  Ankle boot       0.93      0.97      0.95      1000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.86     10000\n",
      "weighted avg       0.87      0.87      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "Y_test_preds = model.apply(final_weights, X_test)\n",
    "Y_test_preds = jnp.argmax(Y_test_preds, axis=1)\n",
    "\n",
    "print(\"Test Accuracy : {}\".format(accuracy_score(Y_test, Y_test_preds)))\n",
    "print(\"\\nConfusion Matrix : \")\n",
    "print(confusion_matrix(Y_test, Y_test_preds))\n",
    "print(\"\\nClassification Report :\")\n",
    "print(classification_report(Y_test, Y_test_preds, target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 6,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# `ToTensor` converts the image data from PIL type to 32-bit floating point\n",
    "# tensors. It divides all numbers by 255 so that all pixel values are between\n",
    "# 0 and 1\n",
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "Fashion-MNIST consists of images from 10 categories, each represented\n",
    "by 6000 images in the training dataset and by 1000 in the test dataset.\n",
    "A *test dataset* (or *test set*) is used for evaluating  model performance and not for training.\n",
    "Consequently the training set and the test set\n",
    "contain 60000 and 10000 images, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "len(mnist_train), len(mnist_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 11
   },
   "source": [
    "The height and width of each input image are both 28 pixels.\n",
    "Note that the dataset consists of grayscale images, whose number of channels is 1.\n",
    "For brevity, throughout this book\n",
    "we store the shape of any image with height $h$ width $w$ pixels as $h \\times w$ or ($h$, $w$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 12,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "mnist_train[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 13
   },
   "source": [
    "[~~Two utility functions to visualize the dataset~~]\n",
    "\n",
    "The images in Fashion-MNIST are associated with the following categories:\n",
    "t-shirt, trousers, pullover, dress, coat, sandal, shirt, sneaker, bag, and ankle boot.\n",
    "The following function converts between numeric label indices and their names in text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 14,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def get_fashion_mnist_labels(labels):  #@save\n",
    "    \"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\n",
    "    text_labels = [\n",
    "        't-shirt', 'trouser', 'pullover', 'dress', 'coat','sandal', 'shirt', 'sneaker', 'bag', 'ankle boot'\n",
    "    ]\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "We can now create a function to visualize these examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            # Tensor Image\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            # PIL Image\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "Here are [**the images and their corresponding labels**] (in text)\n",
    "for the first few examples in the training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))\n",
    "show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "## Reading a Minibatch\n",
    "\n",
    "To make our life easier when reading from the training and test sets,\n",
    "we use the built-in data iterator rather than creating one from scratch.\n",
    "Recall that at each iteration, a data iterator\n",
    "[**reads a minibatch of data with size `batch_size` each time.**]\n",
    "We also randomly shuffle the examples for the training data iterator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "def get_dataloader_workers():  #@save\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 4\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "                             num_workers=get_dataloader_workers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "Let us look at the time it takes to read the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "timer = d2l.Timer()\n",
    "for X, y in train_iter:\n",
    "    continue\n",
    "f'{timer.stop():.2f} sec'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "## Putting All Things Together\n",
    "\n",
    "Now we define [**the `load_data_fashion_mnist` function\n",
    "that obtains and reads the Fashion-MNIST dataset.**]\n",
    "It returns the data iterators for both the training set and validation set.\n",
    "In addition, it accepts an optional argument to resize images to another shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 30,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None):  #@save\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize: trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\", train=True, transform=trans, download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\", train=False, transform=trans, download=True)\n",
    "    return (\n",
    "        data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=get_dataloader_workers()),\n",
    "        data.DataLoader(mnist_test, batch_size, shuffle=False, num_workers=get_dataloader_workers())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "Below we test the image resizing feature of the `load_data_fashion_mnist` function\n",
    "by specifying the `resize` argument.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "origin_pos": 33,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = load_data_fashion_mnist(32, resize=64)\n",
    "for X, y in train_iter:\n",
    "    print(X.shape, X.dtype, y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 34
   },
   "source": [
    "We are now ready to work with the Fashion-MNIST dataset in the sections that follow.\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Fashion-MNIST is an apparel classification dataset consisting of images representing 10 categories. We will use this dataset in subsequent sections and chapters to evaluate various classification algorithms.\n",
    "* We store the shape of any image with height $h$ width $w$ pixels as $h \\times w$ or ($h$, $w$).\n",
    "* Data iterators are a key component for efficient performance. Rely on well-implemented data iterators that exploit high-performance computing to avoid slowing down your training loop.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Does reducing the `batch_size` (for instance, to 1) affect the reading performance?\n",
    "1. The data iterator performance is important. Do you think the current implementation is fast enough? Explore various options to improve it.\n",
    "1. Check out the framework's online API documentation. Which other datasets are available?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "origin_pos": 36,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/49)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
